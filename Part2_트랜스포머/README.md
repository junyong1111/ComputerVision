# PART2 Transformer

## NLP

### RNN (Recurrent Neural Network):
RNN은 순차적인 데이터를 처리하는 데 사용되는 신경망 구조이다. RNN은 이전 시간 단계의 입력을 현재 시간 단계의 입력과 함께 처리하여 순차적인 정보를 유지하고 활용할 수 있고 텍스트, 음성 등 순차적인 시계열 데이터 처리에 유용하다

- 특징: 순차적인 데이터 처리, 이전 상태의 정보를 기억
- 장점: 순차적인 패턴을 학습할 수 있음, 시계열 데이터 처리에 적합
- 단점: 장기 의존성(Long-Term Dependency)을 잘 학습하지 못하는 문제, Gradient Vanishing/Exploding 등의 문제 발생

### LSTM (Long Short-Term Memory):
LSTM은 RNN의 단점 중 하나인 장기 의존성 문제를 해결하기 위해 제안된 변형된 RNN 구조이다. LSTM은 시간적인 의존성을 잘 다룰 수 있도록 설계되었다. LSTM은 게이트를 이용하여 특정 시간 단계에서 중요한 정보를 기억하고, 필요에 따라 이를 장기적으로 전달하거나 삭제할 수 있다.

- 특징: 장기 의존성을 다루기 위한 메모리 셀, 입력 게이트, 삭제 게이트, 출력 게이트 등의 구조
- 장점: 장기 의존성 문제를 해결, 시계열 데이터 처리에 적합
- 단점: 많은 파라미터와 연산이 필요하여 학습과정이 복잡함, 계산량이 크고 처리 속도가 상대적으로 느릴 수 있음

3. Transformer:
Transformer는 자연어 처리(Natural Language Processing) 분야에서 주로 사용되는 모델 구조이다. Attention 메커니즘을 사용하여 입력 시퀀스를 처리하고, 병렬적인 계산을 통해 장기 의존성 문제를 해결한다. Transformer는 인코더와 디코더라는 두 개의 주요 구성 요소로 구성되며, 기계 번역, 챗봇 등 다양한 자연어 처리 작업에 적용된다.

- 특징: Self-Attention 메커니즘, 인코더-디코더 구조
- 장점: 병렬 계산에 용이하여 처리 속도가 빠름, 장기 의존성 문제를 해결, 자연어 처리에 적합
- 단점: 많은 메모리 요구, 학습 데이터 양의 증가에 민감